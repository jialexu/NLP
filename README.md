# NLP 脉络笔记

## 来源一：NLP学习指南：https://github.com/leerumor/nlp_tutorial

系统入门方法
各任务模型list：
各任务综述&技巧：

三个月时间的第一轮学习：
	读懂机器学习、深度学习理论；了解经典任务baseline，动手实践，看懂代码；深入应用场景，尝试修改模型，提升效果。
后续提高要求：
	回归理论，手推公式、盲写模型、拿比赛Top

### 1、基础原理：
	机器学习：线性代数，概率论；
	统计：线性分类、SVM、树模型和图模型；推荐李航《统计学习方法》、吴恩达“CS229公开课”，林田轩“机器学习基石”
	深度学习：吴恩达“深度学习”、李宏毅“深度学习”、邱锡鹏“神经网络与深度学习”，弄懂神经网络的反向传播推导、词向量和其他编码器的核心思想、前向反向过程。

### 2、经典模型与技巧：
	了解NLP各个经典任务的baseline，看懂源代码。快速了解经典任务脉络可以看综述，先了解1、2个任务经典模型再去看，更容易理解。
	文本分类：应用最多且入门必备；
	文本匹配：稍微复杂；
	序列标注：对embedding、编码器、结果推理的模块进行优化；
	文本生成：最复杂的，
	语言模型：很早就有了，但18年BERT崛起后才更被重视。

### 3、实践优化：
	了解任务、看过源代码之后，去当“炼丹师”。不止步于跑通别人github代码，去参加kaggle、天池、Biendata平台的比赛。
	Kaggle有各种kernel可以学习，国内比赛有中文数据，多看顶会论文并复现，做完一个任务后就把任务技巧摸清。


## 各任务模型List汇总：


各任务综述：

文本分类：

Fasttext：便捷工具，包含文本分类和词向量训练。把输入转化为词向量，取平均，在经过线性分类器得到类别。输入的词向量可以是pretrained，也可以随机初始化，跟着分类任务一起训练。
特点：模型本身复杂度低，但能快速产生任务baseline；Fb通过C++实现，提升计算效率；采用char-level的n-gram作为附加特征，解决了长尾词的OOV，也利用n-gram特征提升表现；类别过多时，采用hierarchical softmax进行分类。

TextCNN：Yoon Kim于2014年提出，用CNN编码n-gram特征的首创。很适合中短文本场景的强baseline，不适合长文本，

