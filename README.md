# NLP 脉络笔记

本文档收集了几个来源于网络的NLP入门整理文档，以求较全面的了解NLP知识脉络。

---
# 来源一：NLP学习指南：
> https://github.com/leerumor/nlp_tutorial

## 系统入门方法

### 三个月时间的第一轮学习：
读懂机器学习、深度学习理论；了解经典任务baseline，动手实践，看懂代码；深入应用场景，尝试修改模型，提升效果。
后续提高要求：
回归理论，手推公式、盲写模型、拿比赛Top

### 1、基础原理：
机器学习：线性代数，概率论；
统计：线性分类、SVM、树模型和图模型；推荐李航《统计学习方法》、吴恩达“CS229公开课”，林田轩“机器学习基石”
深度学习：吴恩达“深度学习”、李宏毅“深度学习”、邱锡鹏“神经网络与深度学习”，弄懂神经网络的反向传播推导、词向量和其他编码器的核心思想、前向反向过程。

### 2、经典模型与技巧：
了解NLP各个经典任务的baseline，看懂源代码。快速了解经典任务脉络可以看综述，先了解1、2个任务经典模型再去看，更容易理解。
文本分类：应用最多且入门必备；
文本匹配：稍微复杂；
序列标注：对embedding、编码器、结果推理的模块进行优化；
文本生成：最复杂的，
语言模型：很早就有了，但18年BERT崛起后才更被重视。

### 3、实践优化：
了解任务、看过源代码之后，去当“炼丹师”。不止步于跑通别人github代码，去参加kaggle、天池、Biendata平台的比赛。
Kaggle有各种kernel可以学习，国内比赛有中文数据，多看顶会论文并复现，做完一个任务后就把任务技巧摸清。


## 各任务模型List汇总：

原文为表格形式，没有阅读整理。

## 各任务综述：

### 文本分类：

Fasttext：便捷工具，包含文本分类和词向量训练。把输入转化为词向量，取平均，在经过线性分类器得到类别。输入的词向量可以是pretrained，也可以随机初始化，跟着分类任务一起训练。
特点：模型本身复杂度低，但能快速产生任务baseline；Fb通过C++实现，提升计算效率；采用char-level的n-gram作为附加特征，解决了长尾词的OOV，也利用n-gram特征提升表现；类别过多时，采用hierarchical softmax进行分类。

TextCNN：Yoon Kim于2014年提出，用CNN编码n-gram特征的首创。很适合中短文本场景的强baseline，不适合长文本，


---

# 来源二：NLP入门指南
> https://www.cxyzjd.com/article/zwqjoy/103546648
